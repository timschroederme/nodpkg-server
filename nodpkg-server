#!/usr/bin/env python3

import glob
import json
from datetime import datetime
import requests
from lxml import html
import os
import paramiko
from scp import SCPClient
from jinja2 import Template
import tweepy


DATEFORMAT = "%Y-%m-%d %H:%M:%S"
NOPACKAGES_PATH = "nopackages/*.json"
LATEST_VERSIONS = ".latest-versions"
LATEST_CHECKS = ".latest-checks"
TEMPLATE = "template.jinja2"
REMOTE_PATH = "/opt/nodpkg/index.html"
TEMP_PATH = "/tmp/index.html"
CREDENTIALS_FILE = ".credentials.json"


def scrape(url, xpath):
    response = requests.get(url)
    tree = html.fromstring(response.content)
    path = tree.xpath(xpath + "/text()")
    raw = path[0]

    # If we have several candidates for version strings, choose the last one
    if len(path) > 1:
        for r in path:
            r_clean = r.strip().replace('\n', '').replace('\r', '')
            if len(r_clean) > 0:
                raw = r_clean

    # Remove newlines etc.
    raw_version = raw.strip().replace('\n', ' ').replace('\r', '')

    # Disregard non-digit characters that are not between digits
    if raw_version.find('openssl') == -1:
        first = -1
        last = 0
        for i, c in enumerate(raw_version):
            if c.isdigit():
                if first == -1:
                    first = i
                last = i

        # check if there is "alpha" or "beta" in version string (like with bash)
        nl = raw_version.find('alpha')
        if nl != -1 and (nl + len('alpha')) > last:
            last = nl + len('alpha')
        nl = raw_version.find('beta')
        if nl != -1 and (nl + len('beta')) > last:
            last = nl + len('beta')

        # We have almost what we need
        version = raw_version[first:last + 1]

    else:
        version = raw_version

    # Replace underscore with dot (aka openssl)
    version = version.replace('_', '.')

    return version


def load_latest_versions():
    if os.path.exists(LATEST_VERSIONS):
        with open(LATEST_VERSIONS, 'r') as file:
            return json.loads(file.read())
    else:
        return {}


def store_latest_versions(versions):
    with open(LATEST_VERSIONS, 'w') as file:
        file.write(json.dumps(versions, indent=2))


def load_latest_checks():
    if os.path.exists(LATEST_CHECKS):
        with open(LATEST_CHECKS, 'r') as file:
            return json.loads(file.read())
    else:
        return {}


def store_latest_checks(checks):
    with open(LATEST_CHECKS, 'w') as file:
        file.write(json.dumps(checks, indent=2))


def load_credentials(path):
    if os.path.exists(path):
        with open(path, 'r') as file:
            return json.loads(file.read())
    else:
        return {}


def broadcast_update(consumer_key, consumer_secret, access_token, access_token_secret, message):
    print("  .. publishing to Twitter")
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    api = tweepy.API(auth)
    api.update_status(message)


def update_site(p, host, user):

    print("Updating website..")

    # Generate page
    with open(TEMPLATE, 'r') as file:
        raw = file.read()
    template = Template(raw, trim_blocks=True, lstrip_blocks=True)
    ps = sorted(p, key=lambda k: k['name'])

    variables = {
        "date": datetime.today().strftime('%-d.%m.%Y'),
        "packages": ps
    }
    parsed = template.render(variables)

    # Store page locally
    with open(TEMP_PATH, 'w') as file:
        file.write(parsed)

    # Upload page to web server
    ssh = paramiko.SSHClient()
    ssh.load_system_host_keys()  # Assuming system-wide ssh access is configured
    ssh.connect(hostname=host, username=user, port=22)
    with SCPClient(ssh.get_transport()) as scp:
        scp.put(TEMP_PATH, REMOTE_PATH)

    # Remove local page
    os.remove(TEMP_PATH)


# Main
credentials = load_credentials(CREDENTIALS_FILE)  # Load credentials for website publishing and Twitter
any_change = False
latest_versions = load_latest_versions()
latest_checks = load_latest_checks()
packages = []  # We need that to update the website

# Main Update Loop
for filename in glob.glob(NOPACKAGES_PATH):
    with open(filename) as f:
        data = json.load(f)
        packages.append(data)
        name = data['name']
        print(name + "..", end='')

        # check if project is due for a check
        needs_check = True
        if name in latest_checks:
            stamp = datetime.strptime(latest_checks[name], DATEFORMAT)
            delta = datetime.now() - stamp
            h = divmod(delta.total_seconds(), 3600)[0]  # hours
            if h < 12:  # Don't check a project page more often than every 12 hours
                needs_check = False
                print(" skip")

        if needs_check:
            # check if new release is available
            v = scrape(data['url'], data['xpath'])
            if name in latest_versions:
                if latest_versions[name] == v:
                    print(" ok")
                else:
                    print(" new release")
                    any_change = True
                    latest_versions[name] = v
                    msg = data['name'] + ' was updated to v' + v + ' ' + data['site'] + ' #' + data['name'] + '.'
                    broadcast_update(credentials['twitter-consumer-key'],
                                     credentials['twitter-consumer-secret'],
                                     credentials['twitter-access-token'],
                                     credentials['twitter-access-secret'],
                                     msg)
            else:
                print(" initial check")
                any_change = True
                latest_versions[name] = v
            latest_checks[name] = datetime.now().strftime(DATEFORMAT)

# Store updated metadata
store_latest_versions(latest_versions)
store_latest_checks(latest_checks)

# Update website
if any_change:
    update_site(packages, credentials['server-hostname'], credentials['server-username'])
